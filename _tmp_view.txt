from __future__ import annotations

import json
import os
from typing import Any, Dict, Callable

import openai
from agents import function_tool

# Import the Duffel functions (these should already be written and available)
from map_servers.duffel_server import (
    search_flights,
    create_order,
    create_payment,
    get_order,
    cancel_order,
    get_offer,
    request_order_change_offers,
    confirm_order_change,
)
from map_servers.hotelbeds_server import (
    search_hotels,
    book_hotel,
    get_booking,
    cancel_booking,
)
from map_servers.hotelbeds_store import save_hotel_search_results, save_hotel_images
from map_servers.hotelbeds_server import get_hotel_images_impl
from map_servers.flight_store import save_flight_choice, load_flight_choices
from map_servers.utils import save_user_flight_decision

TOOL_FUNCTIONS = {
    "search_flights": search_flights,
    "save_user_flight_decision": save_user_flight_decision, 
    "create_order" : create_order,
    "create_payment": create_payment,
    "get_order": get_order,
    "cancel_order": cancel_order,
    "get_offer": get_offer,
    "request_order_change_offers": request_order_change_offers,
    "confirm_order_change": confirm_order_change,
    "search_hotels": search_hotels,
    "book_hotel": book_hotel,
    "get_booking": get_booking,
    "cancel_booking": cancel_booking,
    "save_flight_choice": save_flight_choice,
    "load_flight_choices": load_flight_choices,
}

# ----------------------------------------------------------------------
# 1. Configure OpenAI LLM
# ----------------------------------------------------------------------

# OPTION A (recommended): read from environment variable
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError(
        "Please set OPENAI_API_KEY as an environment variable or "
        "hard-code it in agent_app.py before running."
    )

# Set OpenAI API key
openai.api_key = OPENAI_API_KEY

# Create OpenAI client for newer API
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ----------------------------------------------------------------------
# 2. Tool registry: names -> description + Python callables
# ----------------------------------------------------------------------

def _tool_schema() -> Dict[str, Dict[str, Any]]:
    """
    Describe tools in natural language + argument info.
    This is what the LLM sees when deciding which tool to call.
    """
    return {
        "search_flights": {
            "description": "Search for flight offers based on the provided origin, destination, and dates.",
            "args": {
                "slices": "list of { origin: string, destination: string, departure_date: string (YYYY‑MM‑DD) } (required)",
                "passengers": "list of { type: string ('adult'/'child'/'infant') or age: integer } (required)",
                "cabin_class": "string (optional) - 'economy'/'premium_economy'/'business'/'first'",
                "max_connections": "integer (optional) - maximum number of stops per journey",
                "max_offers": "integer (optional)"  
            }
        },
        "create_order": {
            "description": "Create a flight order from a selected offer. Requires passenger identities and contact details.",
            "args": {
                "offer_id": "string (required) - the Duffel offer ID (e.g., 'off_12345').",
                "payment_type": "string (optional) - The payment method to use (default is 'balance').",
                "passengers": "list (required) - A list of passenger details with id, title, gender, given_name, family_name, born_on, email, phone_number.",
                "mode": "string (optional) - The order type: 'instant' or 'hold' (default is 'instant').",
                "create_hold": "boolean (optional) - If True, create a hold order without taking payment (default is False).",
            },
        },
        "create_payment": {
            "description": "Create a payment for an existing order. Supports balance payments and experimental card payments (via payment_source). If amount/currency are missing, it will use the order total.",
            "args": {
                "order_id": "string (required) - Duffel order ID (ord_...).",
                "amount": "string (optional) - amount to pay; defaults to order total.",
                "currency": "string (optional) - currency code; defaults to order currency.",
                "payment_type": "string (optional) - payment method, defaults to 'balance'. Use 'card' when providing payment_source for card payments.",
                "payment_source": "object (optional) - provider-specific fields (e.g., token/payment_method_id) for non-balance payments.",
            },
        },
        "get_order": {
            "description": "Fetch order details including passengers, itinerary, and payments.",
            "args": {
                "order_id": "string (required) - Duffel order ID (ord_...)."
            },
        },
        "get_offer": {
            "description": "Fetch detailed offer info including segments, baggage, cabin, fare brand, and pricing.",
            "args": {
                "offer_id": "string (required) - Duffel offer ID (off_...)."
            },
        },
        "cancel_order": {
            "description": "Request and (optionally) confirm cancellation of an order. Returns refund info when available.",
            "args": {
                "order_id": "string (required) - Duffel order ID (ord_...).",
                "auto_confirm": "boolean (optional) - confirm the cancellation immediately, default true.",
            },
        },
        "request_order_change_offers": {
            "description": "Request change offers for an order (e.g., new dates/routes). Returns priced change offers.",
            "args": {
                "order_id": "string (required) - Duffel order ID (ord_...).",
                "slices": "list (optional) - new journey slices {origin, destination, departure_date} to reprice changes.",
                "max_offers": "integer (optional) - max change offers to return (default 5).",
            },
        },
        "confirm_order_change": {
            "description": "Confirm a change offer. If amount/currency are omitted, it will fetch the change offer to fill them.",
            "args": {
                "order_change_offer_id": "string (required) - Duffel order change offer ID.",
                "payment_type": "string (optional) - payment method (default 'balance').",
                "amount": "string (optional) - change total to pay; defaults from change offer.",
                "currency": "string (optional) - currency; defaults from change offer.",
            },
        },
        "search_hotels": {
            "description": "Search hotel availability via Hotelbeds (test environment by default). Use Hotelbeds destination codes (e.g., PMI, BCN, LON).",
            "args": {
                "destination_code": "string (required) - Hotelbeds destination code (e.g., 'PMI').",
                "check_in": "string (required) - check-in date YYYY-MM-DD.",
                "check_out": "string (required) - check-out date YYYY-MM-DD.",
                "rooms": "list (optional) - occupancy details, e.g., [{'adults':2,'children':0}] or with paxes.",
                "limit": "integer (optional) - max hotels to return (default 5).",
            },
        },
        "book_hotel": {
            "description": "Create a hotel booking via Hotelbeds. Requires rateKey(s) from a search.",
            "args": {
                "holder": "object (required) - {name, surname} of lead guest.",
                "rooms": "list (required) - [{rateKey, paxes: [{roomId, type:'AD'/'CH', name, surname, age}]}].",
                "client_reference": "string (required) - your booking reference.",
                "remark": "string (optional) - special notes.",
            },
        },
        "get_booking": {
            "description": "Retrieve a hotel booking by reference.",
            "args": {
                "reference": "string (required) - booking reference returned by Hotelbeds.",
            },
        },
        "cancel_booking": {
            "description": "Cancel a hotel booking by reference.",
            "args": {
                "reference": "string (required) - booking reference returned by Hotelbeds.",
            },
        },
        "save_flight_choice": {
            "description": "Persist a selected flight offer to local storage for later recall.",
            "args": {
                "choice": "object (required) - flight choice with fields like offer_id, airline, price, currency, cabin_class, origin, destination, departure_date, return_date, passenger_ids",
                "db_path": "string (optional) - sqlite file path, default flight_choices.sqlite"
            },
        },
        "load_flight_choices": {
            "description": "Retrieve recently saved flight choices.",
            "args": {
                "limit": "integer (optional) - number of rows to return (default 10)",
                "db_path": "string (optional) - sqlite file path, default flight_choices.sqlite"
            },
        },
        "save_user_flight_decision": {
            "description": "Save the user's flight selection/choice/decision to a local JSON file.",
            "args": {
                "offer_id": "string (required) - the unique identifier of the flight offer selected by the user.",
                "origin": "string (required) - the IATA code for the origin airport.",
                "destination": "string (required) - the IATA code for the destination airport.",
                "departure_date": "string (required) - the departure date in YYYY-MM-DD format.",
                "return_date": "string (optional) - the return date in YYYY-MM-DD format, if applicable.",
                "cabin_class": "string (optional) - the cabin class selected by the user (default is 'economy').",
                "price": "float (optional) - the price of the selected flight offer (default is 0.0).",
                "currency": "string (optional) - the currency code for the price (default is 'USD').",
            },
        },
    }
# Removed duplicate TOOL_FUNCTIONS definition

# ----------------------------------------------------------------------
# 3. Agent logic: decide tool vs direct answer, then explain
# ----------------------------------------------------------------------

# Initialize the conversation memory list
conversation_history = []

def _truncate(text: str, max_chars: int = 4000) -> str:
    if text is None:
        return ""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n... [truncated]"

def build_system_prompt() -> str:
    tools_desc = _tool_schema()
    tools_text_parts = []
    for name, spec in tools_desc.items():
        tools_text_parts.append(
            f"- {name}:\n"
            f"  description: {spec['description']}\n"
            f"  args: {json.dumps(spec['args'], indent=2)}"
        )
    tools_text = "\n".join(tools_text_parts)

    return (
        "You are a travel assistant that can call a set of tools (Duffel API functions).\n"
        "YOU ONLY ANSWER TRAVEL RELATED QUESTIONS!\n"
        "DONT ANSWER ANYTHING NOT TRAVEL/TOURSIM RELATED!\n"
        "Tools available:\n"
        f"{tools_text}\n\n"
        "You MUST decide if you need to call a tool.\n"
        "If you need a tool, respond ONLY with a JSON object of the form:\n"
        '{\n'
        '  "tool": "<tool_name>",\n'
        '  "args": { ... }\n'
        '}\n'
        "where <tool_name> is one of the tools above, and args contains only simple JSON types.\n"
        "If you can answer directly without tools (e.g., conceptual explanation), respond ONLY with:\n"
        '{ "answer": "<your natural language answer>" }\n'
        "Do not add any extra text outside the JSON. The JSON must be the entire response."
    )

def load_prompt_from_file(prompt_key: str, file_path: str = 'prompts.json') -> str:
    try:
        with open(file_path, 'r') as f:
            prompts = json.load(f)
        return prompts.get(prompt_key, "")
    except FileNotFoundError:
        raise Exception(f"Prompt file '{file_path}' not found.")
    except json.JSONDecodeError:
        raise Exception(f"Error decoding JSON from the prompt file.")

def ask_llm_for_tool_or_answer(user_message: str) -> Dict[str, Any]:
    """
    Step 1: Ask the LLM whether to call a tool, and which one.
    
    Returns parsed JSON dict, either:
      { "answer": "..." }
    or
      { "tool": "<name>", "args": { ... } }
    """
    # Add current user message to conversation history
    conversation_history.append({"role": "user", "content": user_message})

    # Build the system prompt to guide the LLM's behavior
    system_prompt = build_system_prompt()

    # Send the full conversation history + system prompt as context
    messages = [{"role": "system", "content": system_prompt}] + conversation_history[-10:]  # Limit context to last few messages

    # Make the API request with conversation history + system prompt
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # Use a valid model
        messages=messages,
        max_tokens=3000,
    )

    # Extract the response text
    text = response.choices[0].message.content.strip()

    # Add assistant's response to conversation history
    conversation_history.append({"role": "assistant", "content": text})

    try:
        data = json.loads(text)

    except json.JSONDecodeError:
        # Fallback: wrap whatever the model said as a direct answer
        data = {"answer": text}

    return data

def llm_post_tool_response(
    user_message: str,
    tool_name: str,
    args: Dict[str, Any],
    result: Any,
    prompt_key: str = "explain_decision",
    prompt_file: str = "prompts.json"
) -> str:
    """
    Step 3: After calling the tool, ask the LLM to explain the result.
    """
    prompter = load_prompt_from_file(prompt_key, prompt_file)
   
    if not prompter:
        raise ValueError(f"No prompt found with key '{prompt_key}' in {prompt_file}")
    
    # Pre-process variables
    tool_desc = _tool_schema().get(tool_name, {})
    tool_description = tool_desc.get('description', '') if isinstance(tool_desc, dict) else ''
    
    # ✅ FIX: Actually format the prompt with the variables
    formatted_prompt = prompter.format(
        user_message=user_message,
        tool_name=tool_name,
        tool_description=tool_description,
        formatted_args=_truncate(json.dumps(args, indent=2), max_chars=2000),
        formatted_result=_truncate(json.dumps(result, indent=2), max_chars=4000)
    )
    
    messages = [{"role": "system", "content": "You are a helpful flight booking assistant."}] + conversation_history[-10:] + [
        {"role": "user", "content": formatted_prompt},  # ✅ Use formatted_prompt, not raw prompter
    ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=3000,
    )

    text = response.choices[0].message.content.strip()
    # ❌ REMOVE this print to avoid duplicate output
    # print ("this is the text habibi", text)
    conversation_history.append({"role": "assistant", "content": text})
    return text

def handle_user_message(user_message: str) -> str:
    """
    Full agent flow for one user message:
    1. Ask LLM whether to use a tool or answer directly.
    2. If tool: run the Python function, then ask LLM to explain result.
    """
    decision = ask_llm_for_tool_or_answer(user_message)
   
    # Direct answer path
    if "answer" in decision and "tool" not in decision:
        return decision["answer"]

    # Tool path
    tool_name = decision.get("tool")
    args = decision.get("args", {}) or {}

    if tool_name not in TOOL_FUNCTIONS:
        return f"I tried to call an unknown tool '{tool_name}'. Please refine your request."

    tool_fn = TOOL_FUNCTIONS[tool_name]

    try:
        result = tool_fn(**args)
        formatted_result = json.dumps(result, indent=2)
        # Keep tool result in memory, but cap size to avoid blowing context window
        max_chars = 500
        if len(formatted_result) > max_chars:
            formatted_result = formatted_result[:max_chars] + "\n... [truncated]"
        conversation_history.append({"role": "assistant", "content": formatted_result})
        
        if tool_name == "search_hotels":
            # Persist hotel search results for later retrieval
            try:
                save_hotel_search_results(
                    result,
                    destination=args.get("destination_code", ""),
                    check_in=args.get("check_in", ""),
                    check_out=args.get("check_out", ""),
                )
                
            except Exception as persist_err:
                # Do not break user flow if persistence fails
                print(f"Warning: failed to save hotel search results: {persist_err}")
    except TypeError as e:
        return f"There was an error calling tool '{tool_name}' with arguments {args}: {e}"
    except Exception as e:
        return f"Tool '{tool_name}' failed with an exception: {e}"
    if tool_name == "search_flights":
        return llm_post_tool_response(user_message, tool_name, args, result,prompt_key="flight_save", prompt_file="prompts.json")
    return llm_post_tool_response(user_message, tool_name, args, result)

# ----------------------------------------------------------------------
# 4. Simple REPL
# ----------------------------------------------------------------------

def main() -> None:
    print("Flight Assistant (OpenAI model: gpt-3.5-turbo)")
    print("Type 'quit' or 'exit' to stop.\n")

    while True:
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            return

        if user_input.lower() in {"quit", "exit"}:
            print("Goodbye.")
            return

        if not user_input:
            continue

        answer = handle_user_message(user_input)
        print("\nAssistant:\n")
        print(answer)
        
        print("\n---\n")


if __name__ == "__main__":
    main()

